# üè† Gu√≠a: Ejecutar IA de UniversIA Localmente

Esta gu√≠a te muestra c√≥mo desarrollar y probar los servicios de IA sin gastar en APIs externas.

---

## üéØ Opciones para Desarrollo Local

### **Opci√≥n 1: Ollama + LangChain (Recomendado) üî•**

**Ventajas:**
- ‚úÖ 100% gratis y privado
- ‚úÖ No necesita internet
- ‚úÖ Modelos potentes (Llama 3, Mistral, CodeLlama)
- ‚úÖ F√°cil instalaci√≥n
- ‚úÖ Compatible con LangChain

**Desventajas:**
- ‚ö†Ô∏è Requiere GPU (recomendado) o CPU potente
- ‚ö†Ô∏è Modelos m√°s lentos que GPT-4
- ‚ö†Ô∏è ~4-8 GB RAM por modelo

#### Instalaci√≥n:

**Windows:**
```powershell
# Descargar desde: https://ollama.ai/download
# O con winget:
winget install Ollama.Ollama

# Verificar instalaci√≥n
ollama --version
```

**Iniciar servidor Ollama:**
```powershell
# Ollama se ejecuta autom√°ticamente en segundo plano
# Puerto por defecto: http://localhost:11434
```

**Descargar modelos:**
```powershell
# Modelo general (tutor√≠as)
ollama pull llama3:8b          # 4.7 GB - Excelente para chat

# Modelo para c√≥digo (evaluaciones)
ollama pull codellama:13b      # 7.4 GB - Especializado en c√≥digo

# Modelo ligero (desarrollo r√°pido)
ollama pull mistral:7b         # 4.1 GB - R√°pido y eficiente

# Ver modelos instalados
ollama list
```

**Probar modelo:**
```powershell
ollama run llama3:8b "Explica qu√© es un transistor"
```

#### Integraci√≥n con Python (FastAPI + LangChain):

**requirements.txt:**
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
langchain==0.1.0
langchain-community==0.0.10
pydantic==2.5.0
psycopg2-binary==2.9.9
python-dotenv==1.0.0
```

**C√≥digo de ejemplo (tutor-ia-service/main.py):**
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title="UniversIA Tutor IA")

# CORS para Next.js
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configurar Ollama
llm = Ollama(
    model="llama3:8b",
    base_url="http://localhost:11434",
    temperature=0.7,
)

# Template de prompt para tutor
tutor_template = """Eres un tutor de IA experto en educaci√≥n universitaria.

Curso: {course_id}
Estudiante: {student_name}
Progreso del estudiante: {progress}%

Pregunta del estudiante: {message}

Proporciona una respuesta educativa, clara y motivadora. Si es un concepto t√©cnico, 
expl√≠calo con ejemplos pr√°cticos. S√© conciso pero completo.

Respuesta:"""

prompt = PromptTemplate(
    input_variables=["course_id", "student_name", "progress", "message"],
    template=tutor_template
)

chain = LLMChain(llm=llm, prompt=prompt)

# Models
class ChatRequest(BaseModel):
    message: str
    course_id: str
    student_id: str
    session_id: str
    student_name: str
    context: dict = {}

class ChatResponse(BaseModel):
    response: str
    metadata: dict = {}

@app.post("/chat", response_model=ChatResponse)
async def chat_tutor(request: ChatRequest):
    try:
        # Generar respuesta con Ollama
        response = chain.run(
            course_id=request.course_id,
            student_name=request.student_name,
            progress=request.context.get("progress", 0),
            message=request.message
        )
        
        return ChatResponse(
            response=response.strip(),
            metadata={
                "model": "llama3:8b",
                "local": True,
                "tokens_estimate": len(response.split())
            }
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {
        "status": "ok",
        "service": "tutor",
        "model": "llama3:8b (Ollama)",
        "mode": "local"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Ejecutar:**
```powershell
cd tutor-ia-service
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
python main.py
```

**Probar:**
```powershell
curl -X POST http://localhost:8000/chat `
  -H "Content-Type: application/json" `
  -d '{
    "message": "¬øQu√© es un transistor?",
    "course_id": "test",
    "student_id": "test",
    "session_id": "test",
    "student_name": "Test",
    "context": {"progress": 50}
  }'
```

---

### **Opci√≥n 2: GPT4All (Alternativa a Ollama)**

**Instalaci√≥n:**
```powershell
pip install gpt4all
```

**C√≥digo:**
```python
from langchain_community.llms import GPT4All

llm = GPT4All(
    model="mistral-7b-openorca.Q4_0.gguf",
    max_tokens=2048,
)
```

---

### **Opci√≥n 3: LM Studio (GUI para modelos locales)**

**Ventajas:**
- ‚úÖ Interfaz gr√°fica amigable
- ‚úÖ Descarga de modelos f√°cil
- ‚úÖ API compatible con OpenAI
- ‚úÖ No necesitas c√≥digo para probar

**Instalaci√≥n:**
1. Descargar: https://lmstudio.ai/
2. Abrir LM Studio
3. Buscar y descargar modelo (ej: Llama 3 8B)
4. Iniciar servidor local (pesta√±a "Local Server")
5. Puerto: `http://localhost:1234`

**C√≥digo Python (compatible con OpenAI SDK):**
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="http://localhost:1234/v1",  # LM Studio
    api_key="not-needed",
    model="llama-3-8b-instruct",
    temperature=0.7,
)
```

---

### **Opci√≥n 4: APIs Gratuitas (sin instalaci√≥n local)**

#### **Google Gemini (Gratis con l√≠mites generosos)**

**Crear API Key:**
1. https://makersuite.google.com/app/apikey
2. Crear proyecto
3. Copiar API key

**L√≠mites gratis:**
- 60 requests/minuto
- 1,500 requests/d√≠a
- ¬°Suficiente para desarrollo!

**C√≥digo:**
```python
pip install langchain-google-genai

from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-pro",
    google_api_key="TU_API_KEY_AQUI",
    temperature=0.7,
)
```

**.env:**
```env
GEMINI_API_KEY=tu_api_key_de_google
```

#### **Groq (GPT-4 level, gratis)**

**Ventajas:**
- ‚úÖ Rapid√≠simo (m√°s r√°pido que GPT-4)
- ‚úÖ Gratis con l√≠mites altos
- ‚úÖ Modelo Llama 3 70B gratis

**Crear API Key:**
1. https://console.groq.com/
2. Registrarse
3. Crear API key

**C√≥digo:**
```python
pip install langchain-groq

from langchain_groq import ChatGroq

llm = ChatGroq(
    groq_api_key="tu_groq_api_key",
    model_name="llama3-70b-8192",
    temperature=0.7,
)
```

---

## üîÑ Desarrollo H√≠brido (Recomendado)

**Estrategia √≥ptima para desarrollo:**

1. **Desarrollo local:** Ollama (gratis, r√°pido para probar)
2. **Testing:** Gemini/Groq (gratis, mejor calidad)
3. **Producci√≥n:** OpenAI GPT-4 (pagado, m√°xima calidad)

**Configuraci√≥n din√°mica (.env):**
```env
# Desarrollo
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3:8b
OLLAMA_BASE_URL=http://localhost:11434

# Testing
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=tu_key
# GEMINI_MODEL=gemini-pro

# Producci√≥n
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-xxx
# OPENAI_MODEL=gpt-4-turbo
```

**C√≥digo adaptable:**
```python
import os
from dotenv import load_dotenv
from langchain_community.llms import Ollama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

load_dotenv()

def get_llm():
    provider = os.getenv("LLM_PROVIDER", "ollama")
    
    if provider == "ollama":
        return Ollama(
            model=os.getenv("OLLAMA_MODEL", "llama3:8b"),
            base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
        )
    
    elif provider == "gemini":
        return ChatGoogleGenerativeAI(
            model=os.getenv("GEMINI_MODEL", "gemini-pro"),
            google_api_key=os.getenv("GEMINI_API_KEY"),
        )
    
    elif provider == "openai":
        return ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4-turbo"),
            api_key=os.getenv("OPENAI_API_KEY"),
        )
    
    else:
        raise ValueError(f"Provider no soportado: {provider}")

# Usar en tu c√≥digo
llm = get_llm()
```

---

## üöÄ RAG Local (Vector Database)

### **Opci√≥n 1: FAISS (100% local, gratis)**

```powershell
pip install faiss-cpu langchain-community
```

**C√≥digo:**
```python
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings

# Embeddings locales con Ollama
embeddings = OllamaEmbeddings(
    model="llama3:8b",
    base_url="http://localhost:11434"
)

# Crear vector store local
vectorstore = FAISS.from_texts(
    texts=["Contenido del curso 1", "Contenido del curso 2"],
    embedding=embeddings
)

# Guardar para reutilizar
vectorstore.save_local("vectorstore_cursos")

# Cargar
vectorstore = FAISS.load_local(
    "vectorstore_cursos",
    embeddings,
    allow_dangerous_deserialization=True
)

# Buscar
results = vectorstore.similarity_search("transistor", k=3)
```

### **Opci√≥n 2: Chroma (Local, m√°s f√°cil)**

```powershell
pip install chromadb
```

**C√≥digo:**
```python
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3:8b")

vectorstore = Chroma(
    collection_name="cursos_universia",
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)

# Agregar documentos
vectorstore.add_texts(["contenido 1", "contenido 2"])
```

---

## üí∞ Comparaci√≥n de Costos

| Opci√≥n | Costo Desarrollo | Costo Producci√≥n | Velocidad | Calidad |
|--------|-----------------|------------------|-----------|---------|
| **Ollama (local)** | $0 | $0* | Media | Buena |
| **Gemini (gratis)** | $0 | $0-$10/mes | R√°pida | Muy buena |
| **Groq (gratis)** | $0 | $0-$20/mes | Muy r√°pida | Excelente |
| **OpenAI GPT-4** | $30/mes | $100-500/mes | R√°pida | Excelente |

*Ollama requiere servidor con GPU para producci√≥n (~$20-50/mes)

---

## üìã Checklist de Setup Local

### Desarrollo Inicial (Todo gratis):

- [ ] Instalar Ollama
- [ ] Descargar `ollama pull llama3:8b`
- [ ] Descargar `ollama pull codellama:13b`
- [ ] Crear proyecto Python FastAPI
- [ ] Instalar dependencias
- [ ] Configurar LangChain con Ollama
- [ ] Crear vector store FAISS local
- [ ] Probar endpoints
- [ ] Conectar con Next.js (localhost:3000 ‚Üí localhost:8000)

### Cuando funcione local:

- [ ] Registrar en Gemini (gratis)
- [ ] Actualizar c√≥digo para usar Gemini
- [ ] Comparar resultados vs Ollama
- [ ] Decidir modelo para producci√≥n

---

## üêõ Troubleshooting

### Ollama no responde:
```powershell
# Reiniciar Ollama
ollama serve

# Ver logs
Get-Process ollama
```

### Error "Out of memory":
```powershell
# Usar modelo m√°s peque√±o
ollama pull mistral:7b  # En vez de llama3:8b
```

### Respuestas muy lentas:
- Usa GPU si es posible
- Reduce `max_tokens`
- Usa modelo m√°s peque√±o (mistral:7b)
- O cambia a Gemini/Groq (API, m√°s r√°pido)

---

## üéØ Recomendaci√≥n Final

**Para empezar HOY:**

1. **Instala Ollama** (5 minutos)
2. **Descarga Llama 3** (`ollama pull llama3:8b`)
3. **Crea proyecto Python** con el c√≥digo de ejemplo arriba
4. **Prueba localmente** sin gastar nada
5. **Cuando funcione**, migra a Gemini gratis para mejor calidad
6. **En producci√≥n**, eval√∫a si necesitas GPT-4

**Mi setup personal recomendado:**
- Desarrollo: Ollama Llama 3 (gratis, privado)
- Testing: Gemini Pro (gratis, buena calidad)
- Producci√≥n: Groq Llama 3 70B (gratis con l√≠mites, excelente)

---

¬øQuieres que te ayude a configurar Ollama paso a paso? üöÄ
